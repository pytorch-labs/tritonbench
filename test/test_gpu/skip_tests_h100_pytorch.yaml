# Tests we skip in triton-pytorch + OSS CI
# triton-pytorch is the triton version bundled with pytorch nightly
# We need to skip kernels that only work on triton-main
# Usage:
#  op-name: to skip an entire operator
#  op-name:\n\t- impl-name to skip an impl
bf16xint16_gemm:
  # LLVM ERROR: mma16816 data type not supported
  - bf16xint16
flash_attention:
  # thunderkittens cannot handle the default input shapes
  - tk
  # triton_op_flash_v2 will segfault on triton-pytorch
  - triton_op_flash_v2
  # triton_tutorial_* kernels require triton-main
  - triton_tutorial_flash_v2
  - triton_tutorial_flash_v2_opt
  - triton_tutorial_flash_v2_tma
  - triton_tutorial_flash_v2_ws
  - triton_tutorial_flash_v2_tma_ws
  - triton_tutorial_flash_v2_tma_ws_persistent
  - triton_tutorial_flash_v2_bwd_ws
fp8_attention:
  - colfax_fmha
  # triton_flash_v2 requires triton-main
  - triton_flash_v2
# fp8_fused_quant_gemm_rowwise requires fb-only kernels
fp8_fused_quant_gemm_rowwise:
fp8_gemm:
  # triton_*_persistent requires triton-main
  - triton_persistent_fp8_gemm
  - triton_tma_persistent_fp8_gemm
# fbgemm fp8 gemm requires triton-main (desc_helper.fill_2d_tma_descriptor)
fp8_gemm_rowwise:
gemm:
  # triton_*_persistent_* requires triton-main
  - triton_persistent_matmul
  - triton_tma_persistent_matmul
  - triton_tma_persistent_cached_matmul
  - hstu_triton_matmul
  - colfax_cutlass_matmul
# jagged tests are slow, so disable them in OSS
jagged_layer_norm:
jagged_mean:
jagged_softmax:
jagged_sum:
ragged_attention:
  # ../../../lib/Tools/LinearLayout.cpp:565: LinearLayout
  # mlir::triton::LinearLayout::reshapeOuts(ArrayRef<std::pair<StringAttr, int32_t>>) const:
  # Assertion `getTotalOutDimSize() == std::accumulate( newOutDims.begin(), newOutDims.end(),
  # 1, [&](int32_t acc, auto &outDim) { return acc * outDim.second; })' failed.
  - hstu_triton_ragged_attention
  # presistent kernel is not ready for OSS
  - hstu_triton_ragged_attention_persistent
test_op:
